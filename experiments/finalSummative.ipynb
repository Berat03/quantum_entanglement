{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/berat/Desktop/quantum_entanglement/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import deque  # Add this import\n",
    "import math\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumInternet():\n",
    "    def __init__(self, initialEdges, pGen, cutOffAge, goalStates, goalWeights):\n",
    "        self.initialEdges = initialEdges  \n",
    "        self.currentEdges = {} \n",
    "        self.pGen = pGen\n",
    "        self.cutOffAge = cutOffAge\n",
    "        self.goalStates = goalStates\n",
    "        self.goalWeights = goalWeights\n",
    "        self.maxLinks = 1\n",
    "        self.total_timesteps = 0\n",
    "        self.successful_links = {i: 0.0 for i in range(len(goalStates))}\n",
    "    \n",
    "    def get_edrs(self):\n",
    "        \"\"\"Returns current EDR for each goal state\"\"\"\n",
    "        return {\n",
    "            i: self.successful_links[i] / max(1, self.total_timesteps)\n",
    "            for i in range(len(self.goalStates))\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        self.currentEdges = {}\n",
    "        self.globallyGenerateEntanglements()\n",
    "    \n",
    "    def getState(self) -> dict:\n",
    "        return self.currentEdges\n",
    "                \n",
    "    def generateEntanglement(self, node1, node2):\n",
    "        edge = tuple(sorted([node1, node2]))\n",
    "        if edge not in self.currentEdges:\n",
    "            self.currentEdges[edge] = deque([0])\n",
    "        else:\n",
    "            if len(self.currentEdges[edge]) < self.maxLinks:\n",
    "                self.currentEdges[edge].appendleft(0)\n",
    "\n",
    "    def globallyGenerateEntanglements(self):\n",
    "        for edge in self.initialEdges:\n",
    "            if random.random() < self.pGen:\n",
    "                self.generateEntanglement(*edge)\n",
    "    \n",
    "    def discardEntanglement(self, edge: tuple):\n",
    "        if edge in self.currentEdges and len(self.currentEdges[edge]) > 0:\n",
    "            self.currentEdges[edge].pop()\n",
    "        if len(self.currentEdges[edge]) == 0:\n",
    "            del self.currentEdges[edge]\n",
    "                \n",
    "    \n",
    "    def ageEntanglements(self):\n",
    "        edges_to_check = list(self.currentEdges.keys())\n",
    "        for edge in edges_to_check:\n",
    "            newAges = [age + 1 for age in self.currentEdges[edge] if age + 1 <= self.cutOffAge]\n",
    "            self.currentEdges[edge] = deque(newAges)\n",
    "            \n",
    "            if len(self.currentEdges[edge]) == 0:\n",
    "                self.discardEntanglement(edge)\n",
    "                \n",
    "        \n",
    "    def isTerminal(self) -> tuple[bool, list]:\n",
    "        graph = collections.defaultdict(set)\n",
    "        for (a, b) in self.currentEdges:\n",
    "            graph[a].add(b)\n",
    "            graph[b].add(a)\n",
    "        \n",
    "        def has_path(start, end):\n",
    "            if start == end:\n",
    "                return True\n",
    "            \n",
    "            visited = set()\n",
    "            stack = [start]\n",
    "            \n",
    "            while stack:\n",
    "                current = stack.pop()\n",
    "                if current not in visited:\n",
    "                    visited.add(current)\n",
    "                    \n",
    "                    if current == end:\n",
    "                        return True\n",
    "                    \n",
    "                    # Add unvisited neighbors to stack\n",
    "                    stack.extend(\n",
    "                        next_node for next_node in graph[current] \n",
    "                        if next_node not in visited\n",
    "                    )\n",
    "            \n",
    "            return False\n",
    "        \n",
    "        matching = [goal for goal in self.goalStates if has_path(goal[0], goal[-1])]\n",
    "        return bool(matching), matching\n",
    "                \n",
    "    def rewardForAction(self, action): #Returns reward function\n",
    "        pass\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 3): deque([0]), (4, 5): deque([0])}\n",
      "{(1, 3): deque([0]), (3, 4): deque([0]), (4, 5): deque([0])}\n",
      "{(1, 3): deque([0]), (3, 4): deque([0]), (4, 5): deque([0])}\n",
      "(((1, 3), 0), ((3, 4), 0), ((4, 5), 0))\n"
     ]
    }
   ],
   "source": [
    "random.seed(27)\n",
    "initialEdges = [(1,3), (2,3), (3,4), (4,5)]\n",
    "goalStates = [(1, 4), (2,4)]\n",
    "goalWeights = [0.3, 0.7]\n",
    "pGen = 0.7\n",
    "n = 2\n",
    "cutOffAge = 0\n",
    "alpha = 0.3\n",
    "gamma = 0.95  # discount factor\n",
    "epsilon = 0.2  # exploration rate\n",
    "num_episodes = 30000\n",
    "myNetwork = QuantumInternet(initialEdges, pGen, cutOffAge, goalStates, goalWeights)\n",
    "myNetwork.globallyGenerateEntanglements()\n",
    "print(myNetwork.getState())\n",
    "myNetwork.ageEntanglements() # All entanglements are discarded after 1 timestep\n",
    "myNetwork.globallyGenerateEntanglements()\n",
    "print(myNetwork.getState())\n",
    "\n",
    "state = myNetwork.getState()\n",
    "state_key = tuple(\n",
    "            (edge, age[0])\n",
    "            for edge, age in sorted(state.items())\n",
    "        )\n",
    "print(state)\n",
    "print(state_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, state_key, epsilon):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.choice([True, False]) # For swap or not to swap entire connection\n",
    "    else:\n",
    "        # Exploitation: choose action with highest Q-value\n",
    "        if state_key not in Q:\n",
    "            Q[state_key] = {True: 0, False: 0}  # Initialize both actions\n",
    "        \n",
    "        return max(Q[state_key].items(), key=lambda x: x[1])[0]\n",
    "    \n",
    "    Q = {}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 EDRs:\n",
      "Goal (1, 4): EDR = 0.000\n",
      "Goal (2, 4): EDR = 0.000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'next_state_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 98\u001b[0m\n\u001b[1;32m     96\u001b[0m num_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30000\u001b[39m\n\u001b[1;32m     97\u001b[0m myNetwork \u001b[38;5;241m=\u001b[39m QuantumInternet(initialEdges, pGen, cutOffAge, goalStates, goalWeights)\n\u001b[0;32m---> 98\u001b[0m myQ, myEpisodeRewards \u001b[38;5;241m=\u001b[39m \u001b[43mn_step_sarsa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmyNetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 52\u001b[0m, in \u001b[0;36mn_step_sarsa\u001b[0;34m(env, n, alpha, gamma, epsilon, num_episodes)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Convert next_state to hashable format\u001b[39;00m\n\u001b[1;32m     48\u001b[0m state_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m     49\u001b[0m     (edge, age)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m edge, age \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(state\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m     51\u001b[0m )\n\u001b[0;32m---> 52\u001b[0m states\u001b[38;5;241m.\u001b[39mappend(\u001b[43mnext_state_key\u001b[49m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_terminal:\n\u001b[1;32m     55\u001b[0m     T \u001b[38;5;241m=\u001b[39m t \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'next_state_key' is not defined"
     ]
    }
   ],
   "source": [
    "Q = {}\n",
    "\n",
    "def n_step_sarsa(env, n, alpha, gamma, epsilon, num_episodes):\n",
    "    global Q\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        if episode % 100 == 0:\n",
    "            edrs = env.get_edrs()\n",
    "            print(f\"Episode {episode} EDRs:\")\n",
    "            for goal_idx, edr in edrs.items():\n",
    "                print(f\"Goal {env.goalStates[goal_idx]}: EDR = {edr:.3f}\")\n",
    "        \n",
    "        env.reset()\n",
    "        state = env.getState()\n",
    "        # Hashing key to be stored\n",
    "        state_key = tuple(\n",
    "            (edge, age)\n",
    "            for edge, age in sorted(state.items())\n",
    "        )\n",
    "        \n",
    "        action = epsilon_greedy_policy(Q, state_key, epsilon)\n",
    "        \n",
    "        T = float('inf')\n",
    "        t = 0\n",
    "        tau = 0\n",
    "        \n",
    "        # Store states, actions, rewards\n",
    "        states = [state_key]\n",
    "        actions = [action]\n",
    "        rewards = []\n",
    "        \n",
    "        while tau < (T - 1):  # Add step limit check\n",
    "            if t < T:\n",
    "                # Take action and get reward\n",
    "                reward = env.rewardForAction(action)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                # Age entanglements and generate new ones\n",
    "                env.ageEntanglements()\n",
    "                env.globallyGenerateEntanglements()\n",
    "                \n",
    "                # Get next state\n",
    "                next_state = env.getState()\n",
    "                is_terminal, _ = env.isTerminal()\n",
    "                \n",
    "                # Convert next_state to hashable format\n",
    "                state_key = tuple(\n",
    "                    (edge, age)\n",
    "                    for edge, age in sorted(state.items())\n",
    "                )\n",
    "                states.append(next_state_key)\n",
    "                \n",
    "                if is_terminal:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    next_action = epsilon_greedy_policy(Q, next_state_key, epsilon)\n",
    "                    actions.append(next_action)\n",
    "            \n",
    "            tau = t - n + 1\n",
    "            \n",
    "            if tau >= 0:\n",
    "                G = sum([gamma**(i - tau - 1) * rewards[i] for i in range(tau + 1, min(tau + n, T))])\n",
    "                \n",
    "                if tau + n < T:\n",
    "                    if states[tau + n] not in Q:\n",
    "                        Q[states[tau + n]] = {True: 0, False: 0}\n",
    "                    G += gamma**n * Q[states[tau + n]][actions[tau + n]]\n",
    "                \n",
    "                # Update Q-value\n",
    "                if states[tau] not in Q:\n",
    "                    Q[states[tau]] = {True: 0, False: 0}\n",
    "                Q[states[tau]][actions[tau]] += alpha * (G - Q[states[tau]][actions[tau]])\n",
    "                print(Q[states[tau]][actions[tau]])\n",
    "            t += 1\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        \n",
    "        episode_rewards[episode] = sum(rewards)\n",
    "    \n",
    "    return Q, episode_rewards\n",
    "\n",
    "\n",
    "# env\n",
    "random.seed(27)\n",
    "initialEdges = [(1,3), (2,3), (3,4), (4,5)]\n",
    "goalStates = [(1, 4), (2,4)]\n",
    "goalWeights = [0.3, 0.7]\n",
    "pGen = 0.7\n",
    "cutOffAge = 1\n",
    "# sarsa\n",
    "n = 2\n",
    "alpha = 0.3\n",
    "gamma = 0.95  # discount factor\n",
    "epsilon = 0.2  # exploration rate\n",
    "num_episodes = 30000\n",
    "myNetwork = QuantumInternet(initialEdges, pGen, cutOffAge, goalStates, goalWeights)\n",
    "myQ, myEpisodeRewards = n_step_sarsa(myNetwork, n, alpha, gamma, epsilon, num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 highest Q-values:\n",
      "--------------------------------------------------\n",
      "\n",
      "1. Q-value: 0.0000\n",
      "Action: True\n",
      "State:\n",
      "  Edge (1, 3): Ages (0,)\n",
      "  Edge (2, 3): Ages (0,)\n",
      "  Edge (3, 4): Ages (0,)\n",
      "\n",
      "2. Q-value: 0.0000\n",
      "Action: False\n",
      "State:\n",
      "  Edge (1, 3): Ages (0,)\n",
      "  Edge (2, 3): Ages (0,)\n",
      "  Edge (3, 4): Ages (0,)\n",
      "\n",
      "3. Q-value: 0.0000\n",
      "Action: True\n",
      "State:\n",
      "  Edge (1, 3): Ages (0,)\n",
      "  Edge (2, 3): Ages (0,)\n",
      "  Edge (3, 4): Ages (0,)\n",
      "  Edge (4, 5): Ages (0,)\n",
      "  Edge (6, 7): Ages (0,)\n",
      "  Edge (6, 8): Ages (0,)\n",
      "\n",
      "4. Q-value: 0.0000\n",
      "Action: False\n",
      "State:\n",
      "  Edge (1, 3): Ages (0,)\n",
      "  Edge (2, 3): Ages (0,)\n",
      "  Edge (3, 4): Ages (0,)\n",
      "  Edge (4, 5): Ages (0,)\n",
      "  Edge (6, 7): Ages (0,)\n",
      "  Edge (6, 8): Ages (0,)\n",
      "\n",
      "5. Q-value: 0.0000\n",
      "Action: True\n",
      "State:\n",
      "  Edge (1, 3): Ages (0,)\n",
      "  Edge (3, 4): Ages (0,)\n",
      "  Edge (6, 7): Ages (0,)\n",
      "\n",
      "6. Q-value: 0.0000\n",
      "Action: False\n",
      "State:\n",
      "  Edge (1, 3): Ages (0,)\n",
      "  Edge (3, 4): Ages (0,)\n",
      "  Edge (6, 7): Ages (0,)\n",
      "\n",
      "7. Q-value: 0.0000\n",
      "Action: True\n",
      "State:\n",
      "  Edge (1, 3): Ages (0,)\n",
      "  Edge (2, 3): Ages (0,)\n",
      "  Edge (3, 4): Ages (0,)\n",
      "  Edge (4, 5): Ages (0,)\n",
      "  Edge (6, 7): Ages (0,)\n",
      "\n",
      "8. Q-value: 0.0000\n",
      "Action: False\n",
      "State:\n",
      "  Edge (1, 3): Ages (0,)\n",
      "  Edge (2, 3): Ages (0,)\n",
      "  Edge (3, 4): Ages (0,)\n",
      "  Edge (4, 5): Ages (0,)\n",
      "  Edge (6, 7): Ages (0,)\n",
      "\n",
      "9. Q-value: 0.0000\n",
      "Action: True\n",
      "State:\n",
      "  Edge (1, 3): Ages (0,)\n",
      "  Edge (3, 4): Ages (0,)\n",
      "  Edge (4, 5): Ages (0,)\n",
      "  Edge (6, 7): Ages (0,)\n",
      "  Edge (6, 8): Ages (0,)\n",
      "\n",
      "10. Q-value: 0.0000\n",
      "Action: False\n",
      "State:\n",
      "  Edge (1, 3): Ages (0,)\n",
      "  Edge (3, 4): Ages (0,)\n",
      "  Edge (4, 5): Ages (0,)\n",
      "  Edge (6, 7): Ages (0,)\n",
      "  Edge (6, 8): Ages (0,)\n"
     ]
    }
   ],
   "source": [
    "# Get all Q-values in a flat list with their corresponding states and actions\n",
    "q_values = []\n",
    "for state, actions in myQ.items():\n",
    "    for action, value in actions.items():\n",
    "        q_values.append((value, state, action))\n",
    "\n",
    "# Sort by Q-value in descending order and get top 10\n",
    "top_10 = sorted(q_values, key=lambda x: x[0], reverse=True)[:10]\n",
    "\n",
    "# Print the results\n",
    "print(\"Top 10 highest Q-values:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (value, state, action) in enumerate(top_10, 1):\n",
    "    print(f\"\\n{i}. Q-value: {value:.4f}\")\n",
    "    print(f\"Action: {action}\")\n",
    "    print(\"State:\")\n",
    "    for edge, ages in state:\n",
    "        print(f\"  Edge {edge}: Ages {ages}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
