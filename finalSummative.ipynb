{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/berat/Desktop/quantum_entanglement/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import deque  # Add this import\n",
    "import math\n",
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumInternet():\n",
    "    def __init__(self, initialEdges, pGen, cutOffAge, goalStates, goalWeights):\n",
    "        self.initialEdges = initialEdges  \n",
    "        self.currentEdges = {} \n",
    "        self.pGen = pGen\n",
    "        self.cutOffAge = cutOffAge\n",
    "        self.goalStates = goalStates\n",
    "        self.goalWeights = goalWeights\n",
    "        self.maxLinks = 1\n",
    "        # Track EDR for each goal state separately\n",
    "        self.total_timesteps = 0\n",
    "        self.successful_links = {i: 1e-2 for i in range(len(goalStates))}  # Dictionary for each goal\n",
    "    \n",
    "    def get_edrs(self):\n",
    "        \"\"\"Returns current EDR for each goal state\"\"\"\n",
    "        return {\n",
    "            i: self.successful_links[i] / max(1, self.total_timesteps)\n",
    "            for i in range(len(self.goalStates))\n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        self.currentEdges = {}\n",
    "        # Don't reset EDR counters here as they should persist across episodes\n",
    "        self.globallyGenerateEntanglements()\n",
    "        return self.currentEdges\n",
    "    \n",
    "    def reset_edr_stats(self):\n",
    "        \"\"\"Resets EDR tracking counters\"\"\"\n",
    "        self.total_timesteps = 0\n",
    "        self.successful_links = {i: 0 for i in range(len(self.goalStates))}\n",
    "    \n",
    "    def getState(self) -> dict:\n",
    "        return self.currentEdges\n",
    "                \n",
    "    def generateEntanglement(self, node1, node2):\n",
    "        edge = tuple(sorted([node1, node2]))\n",
    "        if edge not in self.currentEdges:\n",
    "            self.currentEdges[edge] = deque([0])\n",
    "        else:\n",
    "            if len(self.currentEdges[edge]) < self.maxLinks:\n",
    "                self.currentEdges[edge].appendleft(0)\n",
    "\n",
    "    def globallyGenerateEntanglements(self):\n",
    "        for edge in self.initialEdges:\n",
    "            if random.random() < self.pGen:\n",
    "                self.generateEntanglement(*edge)\n",
    "    \n",
    "    def discardEntanglement(self, edge: tuple):\n",
    "        if edge in self.currentEdges and len(self.currentEdges[edge]) > 0:\n",
    "            self.currentEdges[edge].pop()\n",
    "            if len(self.currentEdges[edge]) == 0:\n",
    "                del self.currentEdges[edge]\n",
    "    \n",
    "    def ageEntanglements(self):\n",
    "        edges_to_check = list(self.currentEdges.keys())\n",
    "        for edge in edges_to_check:\n",
    "            newAges = [age + 1 for age in self.currentEdges[edge] if age + 1 <= self.cutOffAge]\n",
    "            self.currentEdges[edge] = deque(newAges)\n",
    "            \n",
    "            if len(self.currentEdges[edge]) == 0:\n",
    "                self.discardEntanglement(edge)\n",
    "        \n",
    "    def isTerminal(self) -> tuple[bool, list]:\n",
    "        graph = collections.defaultdict(set)\n",
    "        for (a, b) in self.currentEdges:\n",
    "            graph[a].add(b)\n",
    "            graph[b].add(a)\n",
    "        \n",
    "        def has_path(start, end):\n",
    "            if start == end:\n",
    "                return True\n",
    "            \n",
    "            visited = set()\n",
    "            stack = [start]\n",
    "            \n",
    "            while stack:\n",
    "                current = stack.pop()\n",
    "                if current not in visited:\n",
    "                    visited.add(current)\n",
    "                    \n",
    "                    if current == end:\n",
    "                        return True\n",
    "                    \n",
    "                    # Add unvisited neighbors to stack\n",
    "                    stack.extend(\n",
    "                        next_node for next_node in graph[current] \n",
    "                        if next_node not in visited\n",
    "                    )\n",
    "            \n",
    "            return False\n",
    "        \n",
    "        matching = [goal for goal in self.goalStates if has_path(goal[0], goal[-1])]\n",
    "        return bool(matching), matching\n",
    "                \n",
    "    def rewardForAction(self, action):\n",
    "        self.total_timesteps += 1\n",
    "        if not action:  # If waiting\n",
    "            return -0.001  # Small penalty for waiting\n",
    "            \n",
    "        # Check if we have a successful path\n",
    "        is_terminal, matching = self.isTerminal()\n",
    "        \n",
    "        if not is_terminal:\n",
    "            return -0.001  # Penalty for failed attempt\n",
    "        \n",
    "        # Since we can only achieve one goal at a time, find which goal was matched\n",
    "        for i, goal in enumerate(self.goalStates):\n",
    "            if goal in matching:\n",
    "                self.successful_links[i] += 1\n",
    "                \n",
    "                # Calculate EDRs for all goals\n",
    "                edrs = {j: self.successful_links[j] / max(1, self.total_timesteps) \n",
    "                    for j in range(len(self.goalStates))}\n",
    "                \n",
    "                # Calculate proportional fairness using pÂ²/EDR formula\n",
    "                epsilon = 1e-10  # Small constant to avoid division by zero\n",
    "                reward = 0\n",
    "                for j in range(len(self.goalStates)):\n",
    "                    # Square of weight divided by current EDR for that goal\n",
    "                    reward += self.goalWeights[j] * (self.pGen**2 / (edrs[j]))\n",
    "                \n",
    "                return reward\n",
    "        \n",
    "        return -0.001  # Fallback case\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, state_key, epsilon):\n",
    "    # state_key is already a tuple, no need to convert again\n",
    "    if np.random.rand() < epsilon:\n",
    "        # Exploration: randomly choose between attempting path or waiting\n",
    "        return random.choice([True, False])\n",
    "    else:\n",
    "        # Exploitation: choose action with highest Q-value\n",
    "        if state_key not in Q:\n",
    "            Q[state_key] = {True: 0, False: 0}  # Initialize both actions\n",
    "        \n",
    "        return max(Q[state_key].items(), key=lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 EDRs:\n",
      "Goal (1, 4): EDR = 0.010\n",
      "Goal (2, 4): EDR = 0.010\n",
      "Episode 100 EDRs:\n",
      "Goal (1, 4): EDR = 0.007\n",
      "Goal (2, 4): EDR = 0.010\n",
      "Episode 200 EDRs:\n",
      "Goal (1, 4): EDR = 0.022\n",
      "Goal (2, 4): EDR = 0.014\n",
      "Episode 300 EDRs:\n",
      "Goal (1, 4): EDR = 0.022\n",
      "Goal (2, 4): EDR = 0.015\n",
      "Episode 400 EDRs:\n",
      "Goal (1, 4): EDR = 0.026\n",
      "Goal (2, 4): EDR = 0.020\n",
      "Episode 500 EDRs:\n",
      "Goal (1, 4): EDR = 0.024\n",
      "Goal (2, 4): EDR = 0.020\n",
      "Episode 600 EDRs:\n",
      "Goal (1, 4): EDR = 0.027\n",
      "Goal (2, 4): EDR = 0.020\n",
      "Episode 700 EDRs:\n",
      "Goal (1, 4): EDR = 0.027\n",
      "Goal (2, 4): EDR = 0.024\n",
      "Episode 800 EDRs:\n",
      "Goal (1, 4): EDR = 0.027\n",
      "Goal (2, 4): EDR = 0.024\n",
      "Episode 900 EDRs:\n",
      "Goal (1, 4): EDR = 0.026\n",
      "Goal (2, 4): EDR = 0.023\n",
      "Episode 1000 EDRs:\n",
      "Goal (1, 4): EDR = 0.026\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 1100 EDRs:\n",
      "Goal (1, 4): EDR = 0.026\n",
      "Goal (2, 4): EDR = 0.023\n",
      "Episode 1200 EDRs:\n",
      "Goal (1, 4): EDR = 0.026\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 1300 EDRs:\n",
      "Goal (1, 4): EDR = 0.027\n",
      "Goal (2, 4): EDR = 0.023\n",
      "Episode 1400 EDRs:\n",
      "Goal (1, 4): EDR = 0.026\n",
      "Goal (2, 4): EDR = 0.023\n",
      "Episode 1500 EDRs:\n",
      "Goal (1, 4): EDR = 0.026\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 1600 EDRs:\n",
      "Goal (1, 4): EDR = 0.026\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 1700 EDRs:\n",
      "Goal (1, 4): EDR = 0.025\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 1800 EDRs:\n",
      "Goal (1, 4): EDR = 0.025\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 1900 EDRs:\n",
      "Goal (1, 4): EDR = 0.027\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 2000 EDRs:\n",
      "Goal (1, 4): EDR = 0.028\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 2100 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 2200 EDRs:\n",
      "Goal (1, 4): EDR = 0.028\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 2300 EDRs:\n",
      "Goal (1, 4): EDR = 0.028\n",
      "Goal (2, 4): EDR = 0.023\n",
      "Episode 2400 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 2500 EDRs:\n",
      "Goal (1, 4): EDR = 0.028\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 2600 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 2700 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 2800 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 2900 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 3000 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 3100 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 3200 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 3300 EDRs:\n",
      "Goal (1, 4): EDR = 0.028\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 3400 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 3500 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 3600 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 3700 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 3800 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 3900 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 4000 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 4100 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 4200 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 4300 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 4400 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 4500 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 4600 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 4700 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 4800 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 4900 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 5000 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 5100 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 5200 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 5300 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 5400 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 5500 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 5600 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 5700 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.022\n",
      "Episode 5800 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 5900 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 6000 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 6100 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 6200 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 6300 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 6400 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 6500 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 6600 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 6700 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 6800 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 6900 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 7000 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 7100 EDRs:\n",
      "Goal (1, 4): EDR = 0.029\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 7200 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 7300 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 7400 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 7500 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 7600 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 7700 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 7800 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 7900 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 8000 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 8100 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 8200 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 8300 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 8400 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 8500 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 8600 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 8700 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 8800 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 8900 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 9000 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 9100 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 9200 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 9300 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 9400 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 9500 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 9600 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 9700 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 9800 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n",
      "Episode 9900 EDRs:\n",
      "Goal (1, 4): EDR = 0.030\n",
      "Goal (2, 4): EDR = 0.021\n"
     ]
    }
   ],
   "source": [
    "Q = {}  # State-action value function\n",
    "\n",
    "def n_step_sarsa(env, n, alpha, gamma, epsilon, num_episodes):\n",
    "    global Q\n",
    "    episode_rewards = np.zeros(num_episodes)\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        if episode % 100 == 0:\n",
    "            edrs = env.get_edrs()\n",
    "            print(f\"Episode {episode} EDRs:\")\n",
    "            for goal_idx, edr in edrs.items():\n",
    "                print(f\"Goal {env.goalStates[goal_idx]}: EDR = {edr:.3f}\")\n",
    "        \n",
    "        state = env.reset()\n",
    "        state_key = tuple(\n",
    "            (edge, tuple(ages))\n",
    "            for edge, ages in sorted(state.items())\n",
    "        )\n",
    "        \n",
    "        action = epsilon_greedy_policy(Q, state_key, epsilon)\n",
    "        \n",
    "        T = float('inf')\n",
    "        t = 0\n",
    "        tau = 0\n",
    "        \n",
    "        # Store states, actions, rewards\n",
    "        states = [state_key]\n",
    "        actions = [action]\n",
    "        rewards = []\n",
    "        \n",
    "        while tau < (T - 1):  # Add step limit check\n",
    "            if t < T:\n",
    "                # Take action and get reward\n",
    "                reward = env.rewardForAction(action)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                # Age entanglements and generate new ones\n",
    "                env.ageEntanglements()\n",
    "                env.globallyGenerateEntanglements()\n",
    "                \n",
    "                # Get next state\n",
    "                next_state = env.getState()\n",
    "                is_terminal, _ = env.isTerminal()\n",
    "                \n",
    "                # Convert next_state to hashable format\n",
    "                next_state_key = tuple(\n",
    "                    (edge, tuple(ages))\n",
    "                    for edge, ages in sorted(next_state.items())\n",
    "                )\n",
    "                states.append(next_state_key)\n",
    "                \n",
    "                if is_terminal:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    next_action = epsilon_greedy_policy(Q, next_state_key, epsilon)\n",
    "                    actions.append(next_action)\n",
    "            \n",
    "            tau = t - n + 1\n",
    "            \n",
    "            if tau >= 0:\n",
    "                G = sum([gamma**(i - tau - 1) * rewards[i] for i in range(tau + 1, min(tau + n, T))])\n",
    "                \n",
    "                if tau + n < T:\n",
    "                    if states[tau + n] not in Q:\n",
    "                        Q[states[tau + n]] = {True: 0, False: 0}\n",
    "                    G += gamma**n * Q[states[tau + n]][actions[tau + n]]\n",
    "                \n",
    "                # Update Q-value\n",
    "                if states[tau] not in Q:\n",
    "                    Q[states[tau]] = {True: 0, False: 0}\n",
    "                Q[states[tau]][actions[tau]] += alpha * (G - Q[states[tau]][actions[tau]])\n",
    "            \n",
    "            t += 1\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "        \n",
    "        episode_rewards[episode] = sum(rewards)\n",
    "    \n",
    "    return Q, episode_rewards\n",
    "\n",
    "\n",
    "# env\n",
    "random.seed(27)\n",
    "initialEdges = [(1,3), (2,3), (3,4), (4,5), (6,7), (6,8)]\n",
    "goalStates = [(1, 4), (2,4)]\n",
    "goalWeights = [0.3, 0.7]\n",
    "pGen = 0.3\n",
    "cutOffAge = 1\n",
    "# sarsa\n",
    "n = 1  \n",
    "alpha = 0.1  \n",
    "gamma = 0.90  # discount factor\n",
    "epsilon = 0.1  # exploration rate\n",
    "num_episodes = 10000\n",
    "myNetwork = QuantumInternet(initialEdges, pGen, cutOffAge, goalStates, goalWeights)\n",
    "myQ, myEpisodeRewards = n_step_sarsa(myNetwork, n, alpha, gamma, epsilon, num_episodes)\n",
    "# THE WEIGHTS DID NOT CHANGE ANYTHING!\n",
    "# Infact p^2 really disfavours the longer values signficiantly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
