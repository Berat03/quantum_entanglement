{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import copy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import collections\n",
    "from collections import deque, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self):\n",
    "        self.Q = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    \n",
    "    def _state_to_tuple(self, state):\n",
    "        return tuple(sorted((edge, tuple(sorted(ages))) for edge, ages in state.items()))\n",
    "    \n",
    "    def storeStateActionValue(self, state, action, value):\n",
    "        state_tuple = self._state_to_tuple(state)\n",
    "        action_tuple = tuple(tuple(edge) for edge in action)\n",
    "        self.Q[state_tuple][action_tuple] = value\n",
    "    \n",
    "    def getValue(self, state, action):\n",
    "        state_tuple = self._state_to_tuple(state)\n",
    "        action_tuple = tuple(tuple(edge) for edge in action)\n",
    "        return self.Q[state_tuple][action_tuple]\n",
    "    \n",
    "    def getActionAndValues(self, state):\n",
    "        state_tuple = self._state_to_tuple(state)\n",
    "        return [(action, value) for action, value in self.Q[state_tuple].items()]\n",
    "\n",
    "\n",
    "def epsilonGreedyPolicy(Q, state, epsilon, env):\n",
    "    actions_and_values = Q.getActionAndValues(state)\n",
    "    \n",
    "    if not actions_and_values:\n",
    "        possible_actions = env.getPossibleActions()\n",
    "        return rd.choice(possible_actions)\n",
    "    \n",
    "    if np.random.rand() < epsilon:\n",
    "        return random.choice(actions_and_values)[0]\n",
    "    else:\n",
    "        return max(actions_and_values, key=lambda x: x[1])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QuantumNetwork:\n",
    "    def __init__(self, initialEdges, pGen, pSwap, cutOffAge, maxLinks):\n",
    "        self.initialEdges = initialEdges\n",
    "        self.pGen = pGen\n",
    "        self.pSwap = pSwap\n",
    "        self.cutOffAge = cutOffAge\n",
    "        self.nodeCapacity = nodeCapacity  # Dictionary mapping node -> max entanglements\n",
    "        \n",
    "        self.G = nx.Graph()\n",
    "        nodes = set()\n",
    "        for edge in self.initialEdges:\n",
    "            nodes.add(edge[0])\n",
    "            nodes.add(edge[1])\n",
    "        self.G.add_nodes_from(sorted(nodes))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.G = nx.Graph()\n",
    "        nodes = set()\n",
    "        for edge in self.initialEdges:\n",
    "            nodes.add(edge[0])\n",
    "            nodes.add(edge[1])\n",
    "        self.G.add_nodes_from(nodes)\n",
    "    \n",
    "    def setState(self, state):\n",
    "        self.G = nx.Graph()\n",
    "        for edge, entanglements in state.items():\n",
    "            self.G.add_edge(*edge, entanglements=deque(entanglements))\n",
    "    \n",
    "    def getNodeEntanglementCount(self, node):\n",
    "        \"\"\"Count total entanglements a node is involved in\"\"\"\n",
    "        return sum(len(self.G.edges[e]['entanglements']) \n",
    "                  for e in self.G.edges(node)) if self.G.degree(node) > 0 else 0\n",
    "        \n",
    "    def generateLocalEntanglement(self, node1, node2):\n",
    "        edge = tuple(sorted([node1, node2]))\n",
    "        \n",
    "        # Check if either node has reached its capacity\n",
    "        node1_count = self.getNodeEntanglementCount(node1)\n",
    "        node2_count = self.getNodeEntanglementCount(node2)\n",
    "        \n",
    "        if (node1_count >= self.nodeCapacity[node1] or \n",
    "            node2_count >= self.nodeCapacity[node2]):\n",
    "            return \n",
    "        \n",
    "        if not self.G.has_edge(*edge):\n",
    "            self.G.add_edge(*edge, entanglements=deque([0]))\n",
    "        else:\n",
    "            self.G.edges[edge]['entanglements'].appendleft(0)\n",
    "            \n",
    "    def getState(self):\n",
    "        edge_info = {}\n",
    "        for edge in self.G.edges():\n",
    "            edge_info[edge] = list(self.G.edges[edge]['entanglements']) \n",
    "        return edge_info\n",
    "        \n",
    "    def ageEntanglements(self):\n",
    "        edges_to_check = list(self.G.edges())  # Create a copy of edges to iterate over\n",
    "        for edge in edges_to_check:\n",
    "            entanglements = self.G.edges[edge]['entanglements']\n",
    "            # Remove entanglements that would exceed cutOffAge\n",
    "            while entanglements and entanglements[-1] + 1 > self.cutOffAge:\n",
    "                self.discardEntanglement(edge)\n",
    "            # Age remaining entanglements\n",
    "            if self.G.has_edge(*edge):  # Check if edge still exists\n",
    "                entanglements = self.G.edges[edge]['entanglements']\n",
    "                self.G.edges[edge]['entanglements'] = deque(age + 1 for age in entanglements)\n",
    "    \n",
    "    def generateGlobalEntanglementsProbabalistically(self):\n",
    "        for edge in self.initialEdges:\n",
    "            if rd.random() < self.pGen:\n",
    "                self.generateLocalEntanglement(*edge)\n",
    "    \n",
    "    # def generateLocalEntanglement(self, node1, node2):\n",
    "    #     edge = tuple(sorted([node1, node2]))\n",
    "        \n",
    "    #     node1_entanglements = sum(len(self.G.edges[e]['entanglements']) \n",
    "    #                             for e in self.G.edges(node1)) if self.G.degree(node1) > 0 else 0\n",
    "    #     node2_entanglements = sum(len(self.G.edges[e]['entanglements']) \n",
    "    #                             for e in self.G.edges(node2)) if self.G.degree(node2) > 0 else 0\n",
    "        \n",
    "    #     if node1_entanglements >= self.maxLinks or node2_entanglements >= self.maxLinks:\n",
    "    #         return \n",
    "        \n",
    "    #     if not self.G.has_edge(*edge):\n",
    "    #         self.G.add_edge(*edge, entanglements=deque([0]))\n",
    "    #     else:\n",
    "    #         if len(self.G.edges[edge]['entanglements']) < self.maxLinks:\n",
    "    #             self.G.edges[edge]['entanglements'].appendleft(0)\n",
    "    \n",
    "    def discardEntanglement(self, edge: tuple):\n",
    "        if self.G.has_edge(*edge):\n",
    "            if len(self.G.edges[edge]['entanglements']) > 0:\n",
    "                self.G.edges[edge]['entanglements'].pop()\n",
    "            if len(self.G.edges[edge]['entanglements']) == 0:\n",
    "                self.G.remove_edge(*edge)\n",
    "                \n",
    "         \n",
    "    def drawNetwork(self):\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.clf()\n",
    "        \n",
    "        G_viz = self.G.copy()\n",
    "        G_viz.add_edges_from(self.initialEdges)\n",
    "        \n",
    "        # FIXED POSITION FOR DUMBELL NETWORK\n",
    "        pos = {\n",
    "            0: (-1, 0.5),   # Top left\n",
    "            1: (-1, -0.5),  # Bottom left\n",
    "            2: (0, 0),      # Center\n",
    "            3: (1, 0),      # Center right\n",
    "            4: (2, 0.5),    # Top right\n",
    "            5: (2, -0.5)    # Bottom right\n",
    "        }\n",
    "        # pos = nx.spring_layout(G_viz, seed=27)\n",
    "\n",
    "        nx.draw_networkx_edges(G_viz, pos=pos, \n",
    "                            edgelist=self.initialEdges,\n",
    "                            edge_color='grey',\n",
    "                            style='dashed',\n",
    "                            alpha=0.5)\n",
    "        \n",
    "        for edge in self.G.edges():\n",
    "            entanglements = self.G.edges[edge]['entanglements']\n",
    "            for i, age in enumerate(entanglements):\n",
    "                nx.draw_networkx_edges(G_viz, pos=pos,\n",
    "                                    edgelist=[edge],\n",
    "                                    edge_color='blue',\n",
    "                                    width=2,\n",
    "                                    arrows=True,\n",
    "                                    connectionstyle=f'arc3, rad={-0.1 + i*0.2}')\n",
    "                \n",
    "                edge_x = (pos[edge[0]][0] + pos[edge[1]][0]) / 2\n",
    "                edge_y = (pos[edge[0]][1] + pos[edge[1]][1]) / 2\n",
    "                offset = -0.1 + i*0.2\n",
    "                label_x = edge_x + offset * (pos[edge[1]][1] - pos[edge[0]][1]) / 2\n",
    "                label_y = edge_y - offset * (pos[edge[1]][0] - pos[edge[0]][0]) / 2\n",
    "                plt.text(label_x, label_y, str(age), \n",
    "                        bbox=dict(facecolor='white', edgecolor='lightgray', alpha=0.7))\n",
    "        \n",
    "        nx.draw_networkx_nodes(G_viz, pos=pos, node_color='lightblue')\n",
    "        nx.draw_networkx_labels(G_viz, pos=pos)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: WHAT DO WE DO WITH LINKS AFTER WE'VE FORMED THEM, AS THEY ARE TAKING UP QUBITS BUT WE HAVEN'T USED THEM\n",
    "# TODO: NO ERROR CHECKING FOR SWAPS\n",
    "# TODO: LET DESTROY ENTANGLEMENTS BE A ACTION WE CAN TAKE\n",
    "# TODO: [{'swaps': [], 'goal': (0, 5)}, {'swaps': [], 'goal': None}]\n",
    "# TODO: CURRENTELY SELECTING EVERY SINGLE VALUE\n",
    "class WaitSwapEnvironment(QuantumNetwork):\n",
    "    def __init__(self, initialEdges, pGen, pSwap, cutOffAge, nodeCapacity, goalEdgesAndWeights, nStepsDepth):\n",
    "        super().__init__(initialEdges, pGen, pSwap, cutOffAge, nodeCapacity)\n",
    "        self.goalEdgesAndWeights = goalEdgesAndWeights\n",
    "        self.timestep = 0  # Track total timesteps\n",
    "        self.sumExists = defaultdict(lambda: 0)  # Track sum of times each goal edge exists\n",
    "        self.nStepsDepth = nStepsDepth  # Ensure this line is present\n",
    "        \n",
    "    def getReward(self):\n",
    "        total_reward = 0\n",
    "        \n",
    "        for goal_edge, weight in self.goalEdgesAndWeights:\n",
    "            sorted_goal = tuple(sorted(goal_edge))\n",
    "            \n",
    "            if self.G.has_edge(*sorted_goal):\n",
    "                # Basic, generic version\n",
    "                path = nx.shortest_path(self.G, sorted_goal[0], sorted_goal[1])\n",
    "                num_elementary_edges = len(path) - 1\n",
    "                \n",
    "                instantaneousRate = self.pSwap ** num_elementary_edges\n",
    "                \n",
    "                # Calculate average rate directly\n",
    "                averageRate = (self.sumExists[sorted_goal] / self.timestep) if self.timestep > 0 else 1.0\n",
    "                edge_reward = (instantaneousRate / averageRate) * weight\n",
    "                \n",
    "                total_reward += edge_reward\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    def updateRateParameters(self):\n",
    "        self.timestep += 1\n",
    "        for goal_edge, _ in self.goalEdgesAndWeights:\n",
    "            sorted_goal = tuple(sorted(goal_edge))\n",
    "            self.sumExists[sorted_goal] += int(self.G.has_edge(*sorted_goal))\n",
    "    \n",
    "    def copy(self):\n",
    "        new_env = WaitSwapEnvironment(self.initialEdges, self.pGen, self.pSwap, self.cutOffAge, self.nodeCapacity, self.goalEdgesAndWeights, self.nStepsDepth)\n",
    "        \n",
    "        # Copy the current graph state\n",
    "        new_env.G = self.G.copy()\n",
    "        \n",
    "        # Copy the entanglements for each edge\n",
    "        for edge in new_env.G.edges():\n",
    "            new_env.G.edges[edge]['entanglements'] = deque(self.G.edges[edge]['entanglements'])\n",
    "        \n",
    "        # Copy tracking variables\n",
    "        new_env.timestep = self.timestep\n",
    "        new_env.sumExists = self.sumExists.copy()\n",
    "        \n",
    "        return new_env\n",
    "\n",
    "    def takeAction(self, action):\n",
    "        if action['swaps']: \n",
    "            # Get the ages of all edges involved in swaps\n",
    "            max_age = 0\n",
    "            for edge1, edge2 in action['swaps']:\n",
    "                if self.G.has_edge(*edge1):\n",
    "                    max_age = max(max_age, max(self.G.edges[edge1]['entanglements']))\n",
    "                if self.G.has_edge(*edge2):\n",
    "                    max_age = max(max_age, max(self.G.edges[edge2]['entanglements']))\n",
    "                \n",
    "                self.discardEntanglement(edge1)\n",
    "                self.discardEntanglement(edge2)\n",
    "\n",
    "            if action['goal']:\n",
    "                # Create goal entanglement with the maximum age\n",
    "                edge = tuple(sorted(action['goal']))\n",
    "                if not self.G.has_edge(*edge):\n",
    "                    self.G.add_edge(*edge, entanglements=deque([max_age]))\n",
    "                else:\n",
    "                    self.G.edges[edge]['entanglements'].appendleft(max_age)\n",
    "        \n",
    "        final_state = self.getState()\n",
    "        return final_state\n",
    "        \n",
    "        \n",
    "    def performNStepLookahead(self, n): \n",
    "        # This will get possibleActions and update the Q-values for each action\n",
    "        # We need to pass in the Q-table and the current state\n",
    "        # We need to pass in the n-step lookahead depth\n",
    "        # We need to pass in the gamma value\n",
    "        # We need to pass in the epsilon value\n",
    "        # We need to pass in the nStepsDepth value\n",
    "        # We need to pass in the currentNetwork\n",
    "        # Will update the Q-table for each action, so we can use it the updated values in our epsilonGreedyPolicy\n",
    "        pass\n",
    "    \n",
    "    def recursiveLookahead(network, depth, maxDepth, gamma):\n",
    "        if depth == maxDepth:\n",
    "            return network.getReward()\n",
    "    \n",
    "        possibleActions = network.getPossibleActions()\n",
    "        totalReward = 0\n",
    "        \n",
    "        for action in possibleActions:\n",
    "            tempNetwork = network.copy()\n",
    "            tempNetwork.takeAction(action)\n",
    "            tempNetwork.ageEntanglements()\n",
    "            tempNetwork.generateGlobalEntanglementsProbabalistically()\n",
    "            \n",
    "            reward = network.getReward() + (gamma ** depth) * recursiveLookahead(tempNetwork, depth + 1, maxDepth, gamma)\n",
    "            totalReward += reward\n",
    "        \n",
    "        return totalReward \n",
    "    \n",
    "    def getPossibleActions(self):\n",
    "        possible_actions = []\n",
    "        \n",
    "        for goal_edge, _ in self.goalEdgesAndWeights:\n",
    "            start_node, end_node = goal_edge\n",
    "            \n",
    "            if nx.has_path(self.G, start_node, end_node):\n",
    "                paths = list(nx.all_simple_paths(self.G, start_node, end_node))\n",
    "                for path in paths:\n",
    "                    # Get all edges in the path, always sorted\n",
    "                    path_edges = []\n",
    "                    for i in range(len(path)-1):\n",
    "                        edge = tuple(sorted([path[i], path[i+1]]))\n",
    "                        path_edges.append(edge)\n",
    "                    \n",
    "                    # Check if all edges exist in graph (accounting for either order)\n",
    "                    has_entanglements = all(\n",
    "                        self.G.has_edge(*edge) or self.G.has_edge(*tuple(reversed(edge)))\n",
    "                        for edge in path_edges\n",
    "                    )\n",
    "                    \n",
    "                    if has_entanglements:\n",
    "                        possible_actions.append({\n",
    "                            'swaps': [(edge1, edge2) for edge1, edge2 in zip(path_edges[:-1], path_edges[1:])],\n",
    "                            'goal': goal_edge\n",
    "                        })\n",
    "        \n",
    "        possible_actions.append({\n",
    "            'swaps': [],\n",
    "            'goal': None\n",
    "        })\n",
    "\n",
    "        return possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State:\n",
      "{(0, 2): [0], (1, 2): [0], (2, 3): [0], (3, 4): [0], (3, 5): [0]}\n",
      "Possible Actions:\n",
      "[{'swaps': [((0, 2), (2, 3)), ((2, 3), (3, 5))], 'goal': (0, 5)}, {'swaps': [((1, 2), (2, 3)), ((2, 3), (3, 4))], 'goal': (1, 4)}, {'swaps': [], 'goal': None}]\n",
      "Taking Action: {'swaps': [((0, 2), (2, 3)), ((2, 3), (3, 5))], 'goal': (0, 5)}\n",
      "{(0, 5): [0], (1, 2): [0], (3, 4): [0]}\n",
      "Reward: 0.7\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'recursiveLookahead' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtempNetwork\u001b[38;5;241m.\u001b[39mgetReward()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Test recursive lookahead\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     lookaheadReward \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecursiveLookahead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxDepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLookahead Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlookaheadReward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# ... existing code ...\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[36], line 106\u001b[0m, in \u001b[0;36mWaitSwapEnvironment.recursiveLookahead\u001b[0;34m(network, depth, maxDepth, gamma)\u001b[0m\n\u001b[1;32m    103\u001b[0m     tempNetwork\u001b[38;5;241m.\u001b[39mageEntanglements()\n\u001b[1;32m    104\u001b[0m     tempNetwork\u001b[38;5;241m.\u001b[39mgenerateGlobalEntanglementsProbabalistically()\n\u001b[0;32m--> 106\u001b[0m     reward \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mgetReward() \u001b[38;5;241m+\u001b[39m (gamma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m depth) \u001b[38;5;241m*\u001b[39m \u001b[43mrecursiveLookahead\u001b[49m(tempNetwork, depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, maxDepth, gamma)\n\u001b[1;32m    107\u001b[0m     totalReward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m totalReward\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recursiveLookahead' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# For some reason, the seed needs to be set here, otherwise the results are different every time\n",
    "SEED = 1\n",
    "rd.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Initalise Network\n",
    "# nodeCapacity is initialised manually for now \n",
    "nodeCapacity = {\n",
    "    0: 1,  \n",
    "    1: 1,  \n",
    "    2: 3,  \n",
    "    3: 3, \n",
    "    4: 1,  \n",
    "    5: 1\n",
    "}\n",
    "\n",
    "\n",
    "initialEdges = [\n",
    "    (0, 2),\n",
    "    (1, 2),\n",
    "    (2, 3),\n",
    "    (3, 4),\n",
    "    (3, 5)]\n",
    "goalEdgesAndWeights = [((0,5), 0.7), ((1,4), 0.3)]\n",
    "nStepsDepth = 2\n",
    "gamma = 0.9  # Discount factor\n",
    "maxDepth = 3  # Maximum depth for lookahead\n",
    "\n",
    "network = WaitSwapEnvironment(initialEdges=initialEdges, pGen=0.8,pSwap=1, cutOffAge=1, nodeCapacity=nodeCapacity, \n",
    "                              goalEdgesAndWeights=goalEdgesAndWeights, nStepsDepth=nStepsDepth) \n",
    "# Generate initial entanglements\n",
    "network.generateGlobalEntanglementsProbabalistically()\n",
    "network.generateGlobalEntanglementsProbabalistically()\n",
    "\n",
    "\n",
    "print(\"Initial State:\")\n",
    "print(network.getState())\n",
    "print('Possible Actions:')\n",
    "myActions = network.getPossibleActions()\n",
    "print(myActions)\n",
    "\n",
    "for action in myActions:\n",
    "    print(f'Taking Action: {action}')\n",
    "    tempNetwork = network.copy()\n",
    "    tempNetwork.takeAction(action)\n",
    "    print(tempNetwork.getState())\n",
    "    print(f'Reward: {tempNetwork.getReward()}')\n",
    "\n",
    "    # Test recursive lookahead\n",
    "    lookaheadReward = network.recursiveLookahead( 1, maxDepth, gamma)\n",
    "    print(f'Lookahead Reward: {lookaheadReward}')\n",
    "\n",
    "# ... existing code ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nStepQLearning(network, Q, numEpisodes, n, gamma, epsilon, maxSteps):\n",
    "#     episodeRewards = np.zeros(numEpisodes) \n",
    "#     episodeLengths = np.zeros(numEpisodes)  \n",
    "    \n",
    "#     for episode in range(numEpisodes):\n",
    "#         print(f\"Episode {episode + 1} of {numEpisodes}\")\n",
    "#         # Reset the network, generate initial entanglements\n",
    "#         network.reset()\n",
    "#         network.generateGlobalEntanglementsProbabalistically()\n",
    "#         # Initial States\n",
    "#         state = network.getState() #Â Current State\n",
    "#         actions = epsilonGreedyPolicy(Q, state, epsilon)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
