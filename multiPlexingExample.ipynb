{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/berat/Desktop/quantum_entanglement/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import collections\n",
    "from collections import deque  # Add this import\n",
    "\n",
    "!which python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumNetworksBaseClass():\n",
    "    def __init__(self, initialEdges, pGen, pSwap, cutOffAge, maxLinks, users):\n",
    "        self.initialEdges = copy.deepcopy(initialEdges)  # Don't really need a deep copu\n",
    "        self.currentEdges = {} \n",
    "        self.pGen = pGen\n",
    "        self.pSwap = pSwap\n",
    "        self.cutOffAge = cutOffAge\n",
    "        self.maxLinks = maxLinks\n",
    "        self.users= users\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.currentEdges = {}\n",
    "    \n",
    "    def getState(self) -> dict:\n",
    "        return self.currentEdges\n",
    "        \n",
    "    def generateEntanglements(self, node1, node2): # Extend later to attemptGenerateEntanglements()\n",
    "        edge = tuple(sorted([node1, node2]))\n",
    "        if edge not in self.currentEdges:\n",
    "            self.currentEdges[edge] = deque([0])\n",
    "        else:\n",
    "            if len(self.currentEdges[edge]) < self.maxLinks:\n",
    "                self.currentEdges[edge].appendleft(0) # Retain order by age\n",
    "\n",
    "    def probalisticallyGenerateEntanglements(self):\n",
    "        for edge in self.initialEdges:\n",
    "            if random.random() < self.pGen:\n",
    "                self.generateEntanglements(*edge)\n",
    "    \n",
    "    def discardEntanglement(self, edge: tuple):\n",
    "        # Should never be the case that these aren't the same\n",
    "        if edge in self.currentEdges and len(self.currentEdges[edge]) > 0:\n",
    "            self.currentEdges[edge].pop() # TODO: Assume we have information on the age of the entanglement\n",
    "    \n",
    "    def increaseGlobalEntanglementAge(self):\n",
    "        for edge in self.currentEdges:\n",
    "            newAges = [age + 1 for age in self.currentEdges[edge] if age < self.cutOffAge]\n",
    "            self.currentEdges[edge] = deque(newAges) # Ensure deque! Python type hinting sucks\n",
    "\n",
    "    def performSwapping(self, edge1: tuple, edge2: tuple): # Extend later to attemptSwapping()\n",
    "        # Ensure valid swaps\n",
    "        if edge1 not in self.currentEdges or edge2 not in self.currentEdges:\n",
    "            print(f\"Edge {edge1} or {edge2} not found in currentEdges\")\n",
    "        \n",
    "        if len(self.currentEdges[edge1]) == 0 or len(self.currentEdges[edge2]) == 0:\n",
    "            print(f\"Edge {edge1} or {edge2} has no entanglement\")\n",
    "        \n",
    "        # Swap entanglement with the new edge having ascending order ndoes\n",
    "        combined_nodes = list(edge1) + list(edge2)\n",
    "        unique_nodes = sorted([node for node in combined_nodes if combined_nodes.count(node) == 1])\n",
    "        newLink = tuple(unique_nodes) \n",
    "        old_ages = self.currentEdges.get(edge1, deque([])) + self.currentEdges.get(edge2, deque([]))\n",
    "        newAge = max(old_ages) if old_ages else 0\n",
    "        # Timesteps should not occur during swapping\n",
    "        if newLink in self.currentEdges:\n",
    "            if len(self.currentEdges[newLink]) < self.maxLinks:\n",
    "                self.currentEdges[newLink].append(newAge)\n",
    "        else:\n",
    "            self.currentEdges[newLink] = [newAge]\n",
    "            \n",
    "        self.discardEntanglement(edge1)\n",
    "        self.discardEntanglement(edge2)          \n",
    "        \n",
    "    def getPossibleActionsTimeStepOne(self) -> list: # Only does T=1 steps\n",
    "        active_edges = [edge for edge, ages in self.currentEdges.items() if len(ages) > 0]\n",
    "        \n",
    "        # Find all pairs of edges that share a node (potential swaps)\n",
    "        possible_swaps = []\n",
    "        for edge1, edge2 in itertools.combinations(active_edges, 2):\n",
    "            # Check if edges share a node\n",
    "            if set(edge1) & set(edge2):\n",
    "                possible_swaps.append((edge1, edge2))\n",
    "        \n",
    "        # Add \"no swap\" action\n",
    "        all_actions = [None]  # None represents \"do nothing\"\n",
    "        all_actions.extend(possible_swaps)\n",
    "        \n",
    "        return all_actions\n",
    "    \n",
    "    def getPossibleActions(self) -> list:\n",
    "        def get_possible_swap_sequences(edges_state, depth=0, max_depth=3):\n",
    "            if depth >= max_depth:\n",
    "                return [[]]\n",
    "            \n",
    "            # Get all possible single swaps in current state\n",
    "            possible_swaps = []\n",
    "            active_edges = [edge for edge, ages in edges_state.items() if len(ages) > 0]\n",
    "            \n",
    "            for edge1, edge2 in itertools.combinations(active_edges, 2):\n",
    "                if set(edge1) & set(edge2):  # if edges share a node\n",
    "                    possible_swaps.append((edge1, edge2))\n",
    "            \n",
    "            if not possible_swaps:\n",
    "                return [[]]\n",
    "                    \n",
    "            # For each possible swap, simulate it and recurse\n",
    "            all_sequences = [[]]  # Include empty sequence (no swaps)\n",
    "            for swap in possible_swaps:\n",
    "                # Simulate the swap\n",
    "                edge1, edge2 = swap\n",
    "                # Create new edge from the swap (nodes in ascending order)\n",
    "                combined_nodes = list(edge1) + list(edge2)\n",
    "                new_edge = tuple(sorted([n for n in combined_nodes if combined_nodes.count(n) == 1]))\n",
    "                \n",
    "                # Create new state after swap\n",
    "                new_state = copy.deepcopy(edges_state)\n",
    "                # Remove used edges\n",
    "                if edge1 in new_state and len(new_state[edge1]) > 0:\n",
    "                    new_state[edge1].pop()\n",
    "                if edge2 in new_state and len(new_state[edge2]) > 0:\n",
    "                    new_state[edge2].pop()\n",
    "                \n",
    "                # Clean up empty edges\n",
    "                if edge1 in new_state and len(new_state[edge1]) == 0:\n",
    "                    del new_state[edge1]\n",
    "                if edge2 in new_state and len(new_state[edge2]) == 0:\n",
    "                    del new_state[edge2]\n",
    "                \n",
    "                # Add new edge\n",
    "                if new_edge not in new_state:\n",
    "                    new_state[new_edge] = deque([0])  # Age of new entanglement\n",
    "                elif len(new_state[new_edge]) < self.maxLinks:\n",
    "                    new_state[new_edge].appendleft(0)\n",
    "                    \n",
    "                # Recurse to find subsequent swaps\n",
    "                next_sequences = get_possible_swap_sequences(new_state, depth + 1, max_depth)\n",
    "                # Add current swap to start of each subsequent sequence\n",
    "                for seq in next_sequences:\n",
    "                    all_sequences.append([swap] + seq)\n",
    "            \n",
    "            return all_sequences\n",
    "        \n",
    "        # Get all possible sequences of swaps\n",
    "        all_sequences = get_possible_swap_sequences(self.currentEdges)\n",
    "        return all_sequences\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(1, 3): deque([1]), (3, 4): deque([1]), (4, 5): deque([1]), (4, 6): deque([1]), (2, 3): deque([1])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [((1, 3), (3, 4))],\n",
       " [((1, 3), (3, 4)), ((4, 5), (4, 6))],\n",
       " [((1, 3), (3, 4)), ((4, 5), (1, 4))],\n",
       " [((1, 3), (3, 4)), ((4, 6), (1, 4))],\n",
       " [((1, 3), (2, 3))],\n",
       " [((1, 3), (2, 3)), ((3, 4), (4, 5))],\n",
       " [((1, 3), (2, 3)), ((3, 4), (4, 6))],\n",
       " [((1, 3), (2, 3)), ((4, 5), (4, 6))],\n",
       " [((3, 4), (4, 5))],\n",
       " [((3, 4), (4, 5)), ((1, 3), (2, 3))],\n",
       " [((3, 4), (4, 5)), ((1, 3), (3, 5))],\n",
       " [((3, 4), (4, 5)), ((2, 3), (3, 5))],\n",
       " [((3, 4), (4, 6))],\n",
       " [((3, 4), (4, 6)), ((1, 3), (2, 3))],\n",
       " [((3, 4), (4, 6)), ((1, 3), (3, 6))],\n",
       " [((3, 4), (4, 6)), ((2, 3), (3, 6))],\n",
       " [((3, 4), (2, 3))],\n",
       " [((3, 4), (2, 3)), ((4, 5), (4, 6))],\n",
       " [((3, 4), (2, 3)), ((4, 5), (2, 4))],\n",
       " [((3, 4), (2, 3)), ((4, 6), (2, 4))],\n",
       " [((4, 5), (4, 6))],\n",
       " [((4, 5), (4, 6)), ((1, 3), (3, 4))],\n",
       " [((4, 5), (4, 6)), ((1, 3), (2, 3))],\n",
       " [((4, 5), (4, 6)), ((3, 4), (2, 3))]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Env Parameters\n",
    "cutOffAge = 1\n",
    "pSwap = 1\n",
    "pGen= 0.8\n",
    "maxLinks = 1 # Multiplexing\n",
    "users = [((1,6), 1)] # (user, reward)\n",
    "initialEdges = [(1,3), (2,3), (3,4), (4,5), (4,6)]\n",
    "desiredEdge = [(1,3), (3,4), (4,5), (5,6)]\n",
    "random.seed(1)\n",
    "############################\n",
    "network = QuantumNetworksBaseClass(initialEdges, pGen, pSwap, cutOffAge, maxLinks, users)\n",
    "network.probalisticallyGenerateEntanglements()\n",
    "network.probalisticallyGenerateEntanglements()\n",
    "network.probalisticallyGenerateEntanglements()\n",
    "network.probalisticallyGenerateEntanglements()\n",
    "network.increaseGlobalEntanglementAge()\n",
    "print(network.getState())\n",
    "network.getPossibleActions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m tau \u001b[38;5;241m==\u001b[39m T\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m: \u001b[38;5;66;03m# Can replace while loop later on!\u001b[39;00m\n\u001b[1;32m     42\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mnStepLearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[70], line 29\u001b[0m, in \u001b[0;36mnStepLearning\u001b[0;34m(network, step, numEpisodes, alpha, gamma, epsilon)\u001b[0m\n\u001b[1;32m     27\u001b[0m network\u001b[38;5;241m.\u001b[39mreset() \u001b[38;5;66;03m# No entanglements, is it worth producing a entangelment?\u001b[39;00m\n\u001b[1;32m     28\u001b[0m initialState \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mgetState() \n\u001b[0;32m---> 29\u001b[0m possibleActions \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mgetPossibleActions(\u001b[43minitialState\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m) \n\u001b[1;32m     30\u001b[0m initialAction \u001b[38;5;241m=\u001b[39m epsilon_greedy(Q\u001b[38;5;241m=\u001b[39mQ, state \u001b[38;5;241m=\u001b[39m initialState, possibleActions\u001b[38;5;241m=\u001b[39mpossibleActions, epsilon\u001b[38;5;241m=\u001b[39mepsilon)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Initialize state, action, and reward history for n-step \u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "def epsilon_greedy(Q, state, possibleActions, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        # Choose a random action\n",
    "        return np.random.choice(possibleActions)\n",
    "    else:\n",
    "        # Choose the action with the highest Q-value\n",
    "        q_values = [Q[(state, action)] for action in possibleActions]\n",
    "        return possibleActions[np.argmax(q_values)]\n",
    "\n",
    "network = QuantumNetworksBaseClass(initialEdges, pGen, pSwap, cutOffAge, maxLinks, users)\n",
    "numEpisodes = 5\n",
    "step = 2\n",
    "gamma = 0.99\n",
    "epsilon = 0.1\n",
    "\n",
    "def nStepLearning(network=network, step=step, numEpisodes=10, alpha=0.1, gamma=0.9, epsilon = 0.9):\n",
    "    Q = collections.defaultdict() \n",
    "    \n",
    "    for episode in range(1, numEpisodes + 1):\n",
    "        print(f\"Episode {episode}\")\n",
    "\n",
    "        T = int(1e5)  # Represents the time step at which the episode ends. \n",
    "        tau = 0  # Indicates the time step for the state-action pair to be updated.\n",
    "        t = 0  # Current time step in the episode\n",
    "\n",
    "        # Reset env for a new run and get initial states\n",
    "        network.reset() # No entanglements, is it worth producing a entangelment?\n",
    "        initialState = network.getState() \n",
    "        possibleActions = network.getPossibleActions(initialState[1]) \n",
    "        initialAction = epsilon_greedy(Q=Q, state = initialState, possibleActions=possibleActions, epsilon=epsilon)\n",
    "        \n",
    "        # Initialize state, action, and reward history for n-step \n",
    "        nStates, nAction, nReward = [initialState], [initialAction], [0]\n",
    "        \n",
    "        while True:\n",
    "            if t < T:\n",
    "                pass\n",
    "            tau = t - step + 1\n",
    "            if tau >= 0:\n",
    "                pass\n",
    "            if tau == T-1: # Can replace while loop later on!\n",
    "                break\n",
    "\n",
    " \n",
    "nStepLearning()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
