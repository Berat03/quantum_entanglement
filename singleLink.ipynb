{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import collections\n",
    "from collections import deque  # Add this import\n",
    "#!which python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some functions may not work for multiplexing (such as getPossibleStates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumNetworks():\n",
    "    def __init__(self, initialEdges, pGen, pSwap, cutOffAge, maxLinks, goalEdges):\n",
    "        self.initialEdges = copy.deepcopy(initialEdges) \n",
    "        self.currentEdges = {} \n",
    "        self.pGen = pGen\n",
    "        self.pSwap = pSwap\n",
    "        self.cutOffAge = cutOffAge\n",
    "        self.maxLinks = maxLinks\n",
    "        self.goalEdges = goalEdges\n",
    "        self.total_timesteps = 1\n",
    "        # Extract unique nodes from initialEdges\n",
    "        self.nodes = set(node for edge in initialEdges for node in edge)\n",
    "        # Calculate max qubits per node\n",
    "        self.maxQubits = {node: len([edge for edge in initialEdges if node in edge]) for node in self.nodes}\n",
    "        self.goal_edge_counts = {edge: 0.1 for edge, _ in goalEdges}\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.currentEdges = {}\n",
    "    \n",
    "    def getState(self) -> dict:\n",
    "        return self.currentEdges\n",
    "    \n",
    "        \n",
    "    def generateEntanglement(self, node1, node2): # Extend later to attemptGenerateEntanglements()\n",
    "        edge = tuple(sorted([node1, node2]))\n",
    "        if edge not in self.currentEdges:\n",
    "            self.currentEdges[edge] = deque([0])\n",
    "        else:\n",
    "            if len(self.currentEdges[edge]) < self.maxLinks:\n",
    "                self.currentEdges[edge].appendleft(0) # Retain order by age\n",
    "\n",
    "    def probalisticallyGenerateEntanglements(self):\n",
    "        for edge in self.initialEdges:\n",
    "            if random.random() < self.pGen:\n",
    "                self.generateEntanglement(*edge)\n",
    "    \n",
    "    def discardEntanglement(self, edge: tuple):\n",
    "        try:\n",
    "            self.currentEdges[edge].pop()\n",
    "            if len(self.currentEdges[edge]) == 0:\n",
    "                del self.currentEdges[edge]\n",
    "        except KeyError:\n",
    "            print(f\"Edge {edge} not found in currentEdges, this is might be a problem.\")\n",
    "    \n",
    "    def increaseGlobalEntanglementAge(self):\n",
    "        for edge in list(self.currentEdges.keys()):  # Create a list to avoid modifying dict during iteration\n",
    "            newAges = [age + 1 for age in self.currentEdges[edge] if age < self.cutOffAge]\n",
    "            if not newAges:  # If no ages remain after filtering\n",
    "                del self.currentEdges[edge]  # Remove the empty edge\n",
    "            else:\n",
    "                self.currentEdges[edge] = deque(newAges)\n",
    "        \n",
    "    def performSwapping(self, edge1: tuple, edge2: tuple = None):\n",
    "        # Handle list of swaps\n",
    "        if isinstance(edge1, list):\n",
    "            for swap in edge1:\n",
    "                self.performSwapping(swap[0], swap[1])\n",
    "            return\n",
    "                \n",
    "        # Handle single tuple containing both edges\n",
    "        if isinstance(edge1, tuple) and len(edge1) == 2 and edge2 is None:\n",
    "            edge1, edge2 = edge1\n",
    "        \n",
    "        # Validate edges exist and have entanglements\n",
    "        for edge in (edge1, edge2):\n",
    "            if edge not in self.currentEdges:\n",
    "                print(f\"Edge {edge} not found in currentEdges\")\n",
    "                return\n",
    "            if not self.currentEdges[edge]:\n",
    "                print(f\"Edge {edge} has no entanglement\")\n",
    "                return\n",
    "            \n",
    "        # Check qubit capacity constraints\n",
    "        # node_usage = collections.defaultdict(int)\n",
    "        \n",
    "        # # Count current qubit usage for each node\n",
    "        # for edge, entanglements in self.currentEdges.items():\n",
    "        #     for node in edge:\n",
    "        #         node_usage[node] += len(entanglements)\n",
    "        \n",
    "        # # Check if nodes have enough qubits for the swap\n",
    "        # nodes_involved = set(edge1 + edge2)\n",
    "        # for node in nodes_involved:\n",
    "        #     # Add +1 to account for the new link that will be created\n",
    "        #     if node_usage[node] + 1 > self.maxQubits[node]:\n",
    "        #         print(f\"Node {node} exceeds maximum qubit capacity\")\n",
    "        #         return\n",
    "        \n",
    "        # Create new link from non-shared nodes\n",
    "        combined_nodes = list(edge1) + list(edge2)\n",
    "        newLink = tuple(sorted(node for node in combined_nodes \n",
    "                            if combined_nodes.count(node) == 1))\n",
    "        \n",
    "        # Calculate new age as maximum of consumed entanglement ages\n",
    "        newAge = max(max(self.currentEdges[edge1]), \n",
    "                    max(self.currentEdges[edge2]))\n",
    "        \n",
    "        # Add new entanglement\n",
    "        if newLink not in self.currentEdges:\n",
    "            self.currentEdges[newLink] = deque([newAge])\n",
    "        elif len(self.currentEdges[newLink]) < self.maxLinks:\n",
    "            self.currentEdges[newLink].append(newAge)\n",
    "            \n",
    "        # Consume used entanglements\n",
    "        self.discardEntanglement(edge1)\n",
    "        self.discardEntanglement(edge2) \n",
    "\n",
    "    def calcReward(self, state):\n",
    "        reward = 0\n",
    "        for goal_edge, weight in self.goalEdges:\n",
    "            if goal_edge in state:\n",
    "                goal_edge_edr = self.goal_edge_counts[goal_edge] / self.total_timesteps\n",
    "                reward += weight / goal_edge_edr\n",
    "        return reward\n",
    "    \n",
    "    def isTerminal(self):\n",
    "        for goal_edge, weight in self.goalEdges:\n",
    "            if goal_edge in self.currentEdges:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def getPossibleActionsAndCorrespondingStates(self) -> list:\n",
    "        active_edges = [edge for edge, ages in self.currentEdges.items() if ages and min(ages) < self.cutOffAge - 1] \n",
    "        possible_swaps = []\n",
    "        for edge1, edge2 in itertools.combinations(active_edges, 2):\n",
    "            if set(edge1) & set(edge2):  # if edges share a node\n",
    "                possible_swaps.append((edge1, edge2))\n",
    "        \n",
    "        # Modified to include states and their corresponding swap sequences\n",
    "        possible_states_and_swaps = [(self.currentEdges, [])]  # (state, swap_sequence)\n",
    "        \n",
    "        # Consider sequential swaps up to a reasonable depth (e.g., 3)\n",
    "        max_swap_depth = 3\n",
    "        states_to_explore = [(copy.deepcopy(self.currentEdges), [])]\n",
    "        \n",
    "        for depth in range(max_swap_depth):\n",
    "            next_states = []\n",
    "            for current_state, swap_history in states_to_explore:\n",
    "                # Find possible swaps in current state\n",
    "                active_edges = [edge for edge, ages in current_state.items() if len(ages) > 0]\n",
    "                current_possible_swaps = []\n",
    "                for edge1, edge2 in itertools.combinations(active_edges, 2):\n",
    "                    if set(edge1) & set(edge2):\n",
    "                        current_possible_swaps.append((edge1, edge2))\n",
    "                \n",
    "                # Try each possible swap\n",
    "                for swap in current_possible_swaps:\n",
    "                    edge1, edge2 = swap\n",
    "                    new_state = copy.deepcopy(current_state)\n",
    "                    \n",
    "                    # Perform the swap\n",
    "                    combined_nodes = list(edge1) + list(edge2)\n",
    "                    new_edge = tuple(sorted([n for n in combined_nodes \n",
    "                                        if combined_nodes.count(n) == 1]))\n",
    "                    \n",
    "                    # Remove old edges\n",
    "                    new_state[edge1].pop()\n",
    "                    new_state[edge2].pop()\n",
    "                    if len(new_state[edge1]) == 0: del new_state[edge1]\n",
    "                    if len(new_state[edge2]) == 0: del new_state[edge2]\n",
    "                    \n",
    "                    # Add new edge\n",
    "                    if new_edge not in new_state:\n",
    "                        new_state[new_edge] = deque([0])\n",
    "                    else:\n",
    "                        new_state[new_edge].appendleft(0)\n",
    "                    \n",
    "                    new_swap_history = swap_history + [swap]\n",
    "                    next_states.append((new_state, new_swap_history))\n",
    "                    possible_states_and_swaps.append((new_state, new_swap_history))\n",
    "            \n",
    "            states_to_explore = next_states\n",
    "        \n",
    "        return possible_states_and_swaps\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stateActionToKey(state, action=None): # For Q-learning\n",
    "    state_tuple = tuple(sorted((edge, tuple(ages)) for edge, ages in state.items()))\n",
    "    if action is None or action == []:\n",
    "        return state_tuple\n",
    "    if action: \n",
    "        action_tuple = tuple(sorted(tuple(sorted(edge_pair)) for edge_pair in action))\n",
    "    else:\n",
    "        action_tuple = None\n",
    "    \n",
    "    return (state_tuple, action_tuple)\n",
    "        \n",
    "def epsilonGreedyPolicy(Q, state, epsilon):\n",
    "    possible_states_actions = network.getPossibleActionsAndCorrespondingStates()\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(possible_states_actions) if possible_states_actions else (state, None)\n",
    "    \n",
    "    else:\n",
    "        max_q = float('-inf')\n",
    "        best_state_action = (state, None)\n",
    "        for state_action in possible_states_actions:\n",
    "            state_action_key = stateActionToKey(state, state_action[1])\n",
    "            q_value = Q[state_action_key]\n",
    "            if q_value > max_q:\n",
    "                max_q = q_value\n",
    "                best_state_action = state_action\n",
    "        if max_q == float('-inf'):  # If no actions found in Q-table\n",
    "            return random.choice(possible_states_actions) if possible_states_actions else (state, None)\n",
    "            \n",
    "        return best_state_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 of 100\n",
      "Edge (4, 6) not found in currentEdges\n",
      "Episode 2 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 3 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 4 of 100\n",
      "Episode 5 of 100\n",
      "Episode 6 of 100\n",
      "Episode 7 of 100\n",
      "Episode 8 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 9 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 10 of 100\n",
      "Episode 11 of 100\n",
      "Episode 12 of 100\n",
      "Episode 13 of 100\n",
      "Episode 14 of 100\n",
      "Episode 15 of 100\n",
      "Episode 16 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 17 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 18 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 19 of 100\n",
      "Edge (4, 6) not found in currentEdges\n",
      "Episode 20 of 100\n",
      "Episode 21 of 100\n",
      "Episode 22 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 23 of 100\n",
      "Edge (2, 3) not found in currentEdges\n",
      "Episode 24 of 100\n",
      "Episode 25 of 100\n",
      "Episode 26 of 100\n",
      "Edge (4, 6) not found in currentEdges\n",
      "Edge (4, 5) not found in currentEdges\n",
      "Episode 27 of 100\n",
      "Episode 28 of 100\n",
      "Episode 29 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 30 of 100\n",
      "Episode 31 of 100\n",
      "Edge (4, 5) not found in currentEdges\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 32 of 100\n",
      "Edge (4, 6) not found in currentEdges\n",
      "Edge (2, 3) not found in currentEdges\n",
      "Edge (4, 6) not found in currentEdges\n",
      "Episode 33 of 100\n",
      "Episode 34 of 100\n",
      "Edge (2, 3) not found in currentEdges\n",
      "Episode 35 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 36 of 100\n",
      "Episode 37 of 100\n",
      "Episode 38 of 100\n",
      "Episode 39 of 100\n",
      "Episode 40 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 41 of 100\n",
      "Edge (2, 3) not found in currentEdges\n",
      "Edge (2, 3) not found in currentEdges\n",
      "Episode 42 of 100\n",
      "Edge (2, 3) not found in currentEdges\n",
      "Episode 43 of 100\n",
      "Episode 44 of 100\n",
      "Episode 45 of 100\n",
      "Episode 46 of 100\n",
      "Episode 47 of 100\n",
      "Episode 48 of 100\n",
      "Edge (4, 5) not found in currentEdges\n",
      "Episode 49 of 100\n",
      "Edge (2, 3) not found in currentEdges\n",
      "Episode 50 of 100\n",
      "Episode 51 of 100\n",
      "Episode 52 of 100\n",
      "Episode 53 of 100\n",
      "Episode 54 of 100\n",
      "Episode 55 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 56 of 100\n",
      "Episode 57 of 100\n",
      "Episode 58 of 100\n",
      "Episode 59 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Edge (1, 3) not found in currentEdges\n",
      "Episode 60 of 100\n",
      "Episode 61 of 100\n",
      "Episode 62 of 100\n",
      "Episode 63 of 100\n",
      "Episode 64 of 100\n",
      "Episode 65 of 100\n",
      "Episode 66 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Edge (2, 3) not found in currentEdges\n",
      "Episode 67 of 100\n",
      "Episode 68 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 69 of 100\n",
      "Episode 70 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Edge (2, 3) not found in currentEdges\n",
      "Episode 71 of 100\n",
      "Edge (4, 6) not found in currentEdges\n",
      "Episode 72 of 100\n",
      "Episode 73 of 100\n",
      "Episode 74 of 100\n",
      "Episode 75 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 76 of 100\n",
      "Episode 77 of 100\n",
      "Episode 78 of 100\n",
      "Episode 79 of 100\n",
      "Episode 80 of 100\n",
      "Edge (4, 6) not found in currentEdges\n",
      "Episode 81 of 100\n",
      "Episode 82 of 100\n",
      "Episode 83 of 100\n",
      "Edge (2, 3) not found in currentEdges\n",
      "Episode 84 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 85 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 86 of 100\n",
      "Episode 87 of 100\n",
      "Episode 88 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 89 of 100\n",
      "Edge (1, 3) not found in currentEdges\n",
      "Episode 90 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 91 of 100\n",
      "Episode 92 of 100\n",
      "Episode 93 of 100\n",
      "Episode 94 of 100\n",
      "Episode 95 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Edge (2, 3) not found in currentEdges\n",
      "Episode 96 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 97 of 100\n",
      "Edge (3, 4) not found in currentEdges\n",
      "Episode 98 of 100\n",
      "Episode 99 of 100\n",
      "Episode 100 of 100\n",
      "Edge (3, 4) not found in currentEdges\n"
     ]
    }
   ],
   "source": [
    "# Env Parameters\n",
    "cutOffAge = 1\n",
    "pSwap = 1\n",
    "pGen= 0.8\n",
    "maxLinks = 1 # Disable for now multiplexing ## Off for now\n",
    "goalEdges = [((1,5), 1)] # (user, weight)\n",
    "initialEdges = [(1,3), (2,3), (3,4), (4,5), (4,6)]\n",
    "random.seed(1)\n",
    "network = QuantumNetworks(initialEdges, pGen, pSwap, cutOffAge, maxLinks, goalEdges)\n",
    "Q = collections.defaultdict(lambda: random.uniform(0.0, 0.01))  # Optimistic initialization\n",
    "n = 3\n",
    "epsilon = 0.1 \n",
    "gamma = 0.9\n",
    "alpha = 0.8\n",
    "numEpisodes = 100\n",
    "maxSteps = 100\n",
    "Q = collections.defaultdict(lambda: random.uniform(0.001, 0.01)) \n",
    "\n",
    "episodeRewards = np.zeros(numEpisodes)  # was: episodeRewards = []\n",
    "episodeLengths = np.zeros(numEpisodes)  # was: episodeLengths = []\n",
    "\n",
    "for episode in range(numEpisodes):\n",
    "    print(f\"Episode {episode + 1} of {numEpisodes}\")\n",
    "    network.reset()\n",
    "    network.probalisticallyGenerateEntanglements()\n",
    "    \n",
    "    # Changed: Simplified state/action initialization\n",
    "    state = network.getState()  # was: states = [network.getState()]\n",
    "    _, action = epsilonGreedyPolicy(Q, state, epsilon)  # was: next_state, action = epsilonGreedyPolicy(Q, states[0], epsilon)\n",
    "    \n",
    "    # Changed: Initialize lists after getting initial state/action\n",
    "    states = [state]\n",
    "    actions = [action]\n",
    "    rewards = []\n",
    "    \n",
    "    T = float('inf')\n",
    "    t = 0    \n",
    "    # Changed: Simplified while condition\n",
    "    while t < T:  # was: while tau < (T-1)\n",
    "        # Take action and observe next state and reward\n",
    "        if t < T:\n",
    "            if actions[t] is not None:\n",
    "                network.performSwapping(actions[t])\n",
    "            network.increaseGlobalEntanglementAge()\n",
    "            network.probalisticallyGenerateEntanglements()\n",
    "            \n",
    "            next_state = network.getState()\n",
    "            reward = network.calcReward(next_state)\n",
    "            \n",
    "            states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            # Changed: Combined terminal conditions and simplified action selection\n",
    "            if network.isTerminal() or t >= maxSteps:  # Added maxSteps check\n",
    "                T = t + 1\n",
    "            else:\n",
    "                _, next_action = epsilonGreedyPolicy(Q, next_state, epsilon)\n",
    "                actions.append(next_action)\n",
    "        \n",
    "        # Calculate n-step return and update Q-value\n",
    "        tau = t - n + 1\n",
    "        if tau >= 0:\n",
    "            # Changed: Simplified G calculation\n",
    "            G = sum([gamma**(i - tau - 1) * rewards[i] \n",
    "                    for i in range(tau + 1, min(tau + n, T))])\n",
    "            \n",
    "            # Changed: Simplified Q-value update logic\n",
    "            if tau + n < T:\n",
    "                next_state_action = stateActionToKey(states[tau + n], actions[tau + n])\n",
    "                G += gamma**n * Q[next_state_action]\n",
    "            \n",
    "            current_state_action = stateActionToKey(states[tau], actions[tau])\n",
    "            Q[current_state_action] += alpha * (G - Q[current_state_action])\n",
    "        \n",
    "        t += 1\n",
    "        \n",
    "    # Changed: Use numpy array assignment instead of list indexing\n",
    "    episodeRewards[episode] = sum(rewards)  \n",
    "    episodeLengths[episode] = len(rewards)\n",
    "\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Q-values:\n",
      "\n",
      "1. Q-value: 8.985609427639279\n",
      "   State-Action: (((1, 3), (0,)), ((1, 5), (1,)), ((2, 3), (1,)), ((3, 4), (0,)), ((4, 5), (0,)), ((4, 6), (0,)))\n",
      "\n",
      "2. Q-value: 8.928027204065438\n",
      "   State-Action: ((((1, 3), (0,)), ((1, 5), (1,)), ((2, 3), (1,)), ((3, 4), (0,)), ((4, 5), (0,)), ((4, 6), (0,))), (((2, 3), (3, 4)), ((2, 4), (4, 6))))\n",
      "\n",
      "3. Q-value: 8.928008428880332\n",
      "   State-Action: ((((1, 3), (0,)), ((1, 5), (1,)), ((2, 3), (1,)), ((3, 4), (0,)), ((4, 5), (0,)), ((4, 6), (0,))), (((1, 3), (3, 4)), ((1, 4), (4, 6))))\n",
      "\n",
      "4. Q-value: 8.64039396087698\n",
      "   State-Action: ((((1, 3), (0,)), ((1, 5), (1,)), ((2, 3), (1,)), ((3, 4), (0,)), ((4, 5), (0,))), (((4, 6), (5, 6)),))\n",
      "\n",
      "5. Q-value: 8.640351129046808\n",
      "   State-Action: ((((1, 3), (0,)), ((1, 5), (1,)), ((2, 3), (1,)), ((3, 4), (0,)), ((4, 5), (0,)), ((4, 6), (0,))), (((1, 3), (3, 4)), ((4, 5), (4, 6))))\n",
      "\n",
      "6. Q-value: 8.640324344574127\n",
      "   State-Action: ((((1, 3), (0,)), ((1, 5), (1,)), ((2, 3), (1,)), ((3, 4), (0,)), ((4, 5), (0,)), ((4, 6), (0,))), (((1, 3), (2, 3)), ((4, 5), (4, 6))))\n",
      "\n",
      "7. Q-value: 8.640285647437487\n",
      "   State-Action: ((((1, 3), (0,)), ((1, 5), (1,)), ((2, 3), (1,)), ((3, 4), (0,)), ((4, 5), (0,)), ((4, 6), (0,))), (((1, 3), (3, 4)), ((4, 6), (5, 6))))\n",
      "\n",
      "8. Q-value: 8.640284053733057\n",
      "   State-Action: ((((1, 3), (0,)), ((1, 5), (1,)), ((2, 3), (1,)), ((3, 4), (0,)), ((4, 5), (0,)), ((4, 6), (0,))), (((4, 5), (4, 6)),))\n",
      "\n",
      "9. Q-value: 8.640213654307521\n",
      "   State-Action: ((((1, 3), (0,)), ((1, 5), (1,)), ((2, 3), (1,)), ((4, 5), (0,)), ((4, 6), (0,))), (((1, 3), (3, 4)), ((1, 4), (4, 6))))\n",
      "\n",
      "10. Q-value: 8.640119931883339\n",
      "   State-Action: ((((1, 3), (0,)), ((1, 5), (1,)), ((2, 3), (1,)), ((3, 4), (0,)), ((4, 5), (0,)), ((4, 6), (0,))), (((3, 4), (4, 5)),))\n"
     ]
    }
   ],
   "source": [
    "# Sort Q-values and print top 10\n",
    "sorted_Q = sorted(Q.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 Q-values:\")\n",
    "for i, (state_action, value) in enumerate(sorted_Q[:10], 1):\n",
    "    print(f\"\\n{i}. Q-value: {value}\")\n",
    "    print(f\"   State-Action: {state_action}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
